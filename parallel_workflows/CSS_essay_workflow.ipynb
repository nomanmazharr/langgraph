{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017f563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSSEssayState(TypedDict):\n",
    "    essay: str\n",
    "    analysis_feedback: str\n",
    "    language_feedback: str\n",
    "    clarity_of_thought_feedback: str\n",
    "    overall_feedback: str\n",
    "    individual_scores: Annotated[list[int], operator.add]\n",
    "    avg_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b50b1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(model_name='claude-3-5-sonnet-20241022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4287fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationSchema(BaseModel):\n",
    "    feedback: str = Field(description='feedback for the essay')\n",
    "    score: int = Field(description='Score out of 10 for the essay', ge=0, lt=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a905b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_structured_output = llm.with_structured_output(EvaluationSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e089d2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatAnthropic(model='claude-3-5-sonnet-20241022', anthropic_api_url='https://api.anthropic.com', anthropic_api_key=SecretStr('**********'), model_kwargs={}), kwargs={'tools': [{'name': 'EvaluationSchema', 'input_schema': {'properties': {'feedback': {'description': 'feedback for the essay', 'type': 'string'}, 'score': {'description': 'Score out of 10 for the essay', 'exclusiveMaximum': 10, 'minimum': 0, 'type': 'integer'}}, 'required': ['feedback', 'score'], 'type': 'object'}, 'description': ''}], 'ls_structured_output_format': {'kwargs': {'method': 'function_calling'}, 'schema': {'name': 'EvaluationSchema', 'input_schema': {'properties': {'feedback': {'description': 'feedback for the essay', 'type': 'string'}, 'score': {'description': 'Score out of 10 for the essay', 'exclusiveMaximum': 10, 'minimum': 0, 'type': 'integer'}}, 'required': ['feedback', 'score'], 'type': 'object'}, 'description': ''}}, 'tool_choice': {'type': 'tool', 'name': 'EvaluationSchema'}}, config={}, config_factories=[])\n",
       "| PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.EvaluationSchema'>])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_structured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ed4308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_analysis(state: CSSEssayState) -> CSSEssayState:\n",
    "    essay = state['essay']\n",
    "\n",
    "    prompt = f'Analyse the essay deeply according to the given topic, provide a detailed feedback and also assign a score out of 10 \\n {essay}'\n",
    "    analysis_result = llm_with_structured_output(prompt)\n",
    "\n",
    "    return {'analysis_feedback': analysis_result.feedback, 'individual_scores': [analysis_result.score]}\n",
    "\n",
    "\n",
    "def evaluate_language(state: CSSEssayState) -> CSSEssayState:\n",
    "    essay = state['essay']\n",
    "\n",
    "    prompt = f'Analyse the language of the essay, provide a detailed feedback and also assign a score from 10 on the basis of the language \\n {essay}'\n",
    "    language_result = llm_with_structured_output(prompt)\n",
    "\n",
    "    return {'language_feedback': language_result.feedback, 'individual_scores': language_result.score}\n",
    "\n",
    "\n",
    "def evaluate_thought(state: CSSEssayState) -> CSSEssayState:\n",
    "    essay = state['essay']\n",
    "\n",
    "    prompt = f'Analyse the thought level of the person from the essay, provide a detailed feedback and also assign a score out of 10 of the essay \\n {essay}'\n",
    "    thought_result = llm_with_structured_output(prompt)\n",
    "\n",
    "    return {'clarity_of_thought_feedback': thought_result.feedback, 'individual_scores': thought_result.score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79ebc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_evaluation(state: CSSEssayState) -> CSSEssayState:\n",
    "    essay = state['essay']\n",
    "\n",
    "    prompt = f\"Summarize final feedback on the basis of these feedback \\n Language Feedback: \\n {state['language_feedback']}, \\n Thought Feedback: \\n {state['clarity_of_thought_feedback']} \\n Analysis Feedback \\n {state['analysis_feedback']}\"\n",
    "\n",
    "    overall_feedback = llm.invoke(prompt).content\n",
    "\n",
    "    avg_score = sum(state['individual_scores'])/len(state['individual_scores'])\n",
    "\n",
    "    return {'overall_feedback': overall_feedback, 'avg_score': avg_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f6be4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(CSSEssayState)\n",
    "\n",
    "graph.add_node('evaluate_analysis', evaluate_analysis)\n",
    "graph.add_node('evaluate_language', evaluate_language)\n",
    "graph.add_node('evaluate_thought', evaluate_thought)\n",
    "graph.add_node('final_evaluation', final_evaluation)\n",
    "\n",
    "\n",
    "graph.add_edge(START, \"evaluate_analysis\")\n",
    "graph.add_edge(START, \"evaluate_language\")\n",
    "graph.add_edge(START, \"evaluate_thought\")\n",
    "\n",
    "graph.add_edge(\"evaluate_analysis\", \"final_evaluation\")\n",
    "graph.add_edge(\"evaluate_language\", \"final_evaluation\")\n",
    "graph.add_edge(\"evaluate_thought\", \"final_evaluation\")\n",
    "\n",
    "graph.add_edge(\"final_evaluation\", END)\n",
    "\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7492520",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {'essay': essay}\n",
    "\n",
    "final_state = workflow.invoke(initial_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
